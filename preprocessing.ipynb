{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e7183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c7699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = pd.read_csv('./claim.csv', names=['debt_id', 'collected_amount', 'date_of_trans'], header=0)\n",
    "actions = pd.read_csv('./action.csv', names=['debt_id', 'contact_date'], header=0)\n",
    "collections = pd.read_csv('collection.csv', names=['debt_id', 'collector_id', 'enter_date', 'amount_outstanding', 'amount',\n",
    "       'service_end_date', 'agent_id'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3432b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_droper(X):\n",
    "    \"\"\"\n",
    "    Remove rows with missing values from the DataFrame.\n",
    "\n",
    "    This function creates a copy of the input DataFrame and removes all rows that contain\n",
    "    missing values (NaNs) in any column. The resulting DataFrame will only contain rows\n",
    "    with complete data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The input data from which rows with missing values will be removed. A pandas DataFrame\n",
    "        containing potential NaN values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_transformed : DataFrame\n",
    "        A new DataFrame with rows containing missing values removed. The original DataFrame\n",
    "        remains unchanged.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'A': [1, 2, None, 4],\n",
    "    ...     'B': [None, 2, 3, 4],\n",
    "    ...     'C': [1, None, 3, 4]\n",
    "    ... })\n",
    "    >>> nan_droper(data)\n",
    "       A    B    C\n",
    "    3  4  4.0  4.0\n",
    "    \"\"\"\n",
    "    X_transformed = X.copy()\n",
    "    return X_transformed.dropna()\n",
    "\n",
    "\n",
    "class DropBy(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that drops rows from a DataFrame based on a specified algorithm.\n",
    "\n",
    "    The `DropBy` transformer allows users to remove rows from a DataFrame according to a\n",
    "    custom algorithm provided at initialization. This is particularly useful in scenarios\n",
    "    where specific, complex conditions need to be applied to filter out rows from the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algorithm : callable\n",
    "        A function that takes a DataFrame `X` as input and returns a boolean Series\n",
    "        with the same index as `X`. Rows corresponding to `True` in the Series will\n",
    "        be dropped from the DataFrame during the transformation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None):\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "    transform(X):\n",
    "        Apply the row-dropping algorithm to the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The input data to be transformed. A pandas DataFrame from which rows will\n",
    "        be removed based on the algorithm.\n",
    "\n",
    "    y : None, optional\n",
    "        Ignored. This parameter is included to maintain compatibility with the\n",
    "        scikit-learn transformer API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_transformed : DataFrame\n",
    "        The transformed DataFrame with rows dropped according to the algorithm.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    >>> \n",
    "    >>> # Define a sample algorithm that drops rows where the value in column 'A' is greater than 2\n",
    "    >>> def drop_algorithm(X):\n",
    "    ...     return X['A'] > 2\n",
    "    ...\n",
    "    >>> # Sample data\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'A': [1, 2, 3, 4],\n",
    "    ...     'B': [5, 6, 7, 8]\n",
    "    ... })\n",
    "    ...\n",
    "    >>> # Create the transformer\n",
    "    >>> drop_by_transformer = DropBy(algorithm=drop_algorithm)\n",
    "    ...\n",
    "    >>> # Apply the transformation\n",
    "    >>> transformed_data = drop_by_transformer.fit_transform(data)\n",
    "    >>> print(transformed_data)\n",
    "       A  B\n",
    "    0  1  5\n",
    "    1  2  6\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        Initialize the DropBy transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : callable\n",
    "            A function that takes a DataFrame `X` as input and returns a boolean Series\n",
    "            with the same index as `X`. Rows corresponding to `True` in the Series will\n",
    "            be dropped from the DataFrame during the transformation.\n",
    "        \"\"\"\n",
    "        self.algorithm_ = algorithm\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to fit. Not used in this transformer.\n",
    "\n",
    "        y : None, optional\n",
    "            Ignored. This parameter is included to maintain compatibility with the\n",
    "            scikit-learn transformer API.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DropBy\n",
    "            The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the row-dropping algorithm to the input DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to be transformed. A pandas DataFrame from which rows will\n",
    "            be removed based on the algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : DataFrame\n",
    "            The transformed DataFrame with rows dropped according to the algorithm.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        return X_transformed.drop(X_transformed.loc[self.algorithm_(X)].index)\n",
    "\n",
    "\n",
    "class ToDateTimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that converts specified columns in a DataFrame to datetime format.\n",
    "\n",
    "    This transformer is designed for use in a scikit-learn pipeline to convert one or more\n",
    "    columns in a DataFrame to datetime format. The transformation uses pandas' `pd.to_datetime`\n",
    "    function, with additional parameters passed via `kwargs`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : list of str\n",
    "        Column(s) to convert to datetime. This can be a single column name or a list of column names.\n",
    "\n",
    "    kwargs : keyword arguments, optional\n",
    "        Additional parameters to pass to `pd.to_datetime`. For example, `format`, `errors`, etc.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None):\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "    transform(X):\n",
    "        Convert specified columns to datetime format in the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The input data to be transformed. A pandas DataFrame with columns specified in `cols`.\n",
    "\n",
    "    y : None\n",
    "        Ignored. This parameter is included to maintain compatibility with the\n",
    "        scikit-learn transformer API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_transformed : DataFrame\n",
    "        The transformed DataFrame with specified columns converted to datetime format.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'Date': ['2024-01-01', '2024-02-01', '2024-03-01'],\n",
    "    ...     'Value': [10, 20, 30]\n",
    "    ... })\n",
    "    >>> transformer = ToDateTimeTransformer(cols=['Date'])\n",
    "    >>> transformer.fit_transform(data)\n",
    "         Date  Value\n",
    "    0 2024-01-01     10\n",
    "    1 2024-02-01     20\n",
    "    2 2024-03-01     30\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the ToDateTimeTransformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : list of str\n",
    "            Column(s) to convert to datetime. This can be a single column name or a list of column names.\n",
    "\n",
    "        kwargs : keyword arguments, optional\n",
    "            Additional parameters to pass to `pd.to_datetime`. For example, `format`, `errors`, etc.\n",
    "        \"\"\"\n",
    "        self.cols_ = cols\n",
    "        self.kwargs_ = kwargs\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to fit. Not used in this transformer.\n",
    "\n",
    "        y : None\n",
    "            Ignored. This parameter is included to maintain compatibility with the\n",
    "            scikit-learn transformer API.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : ToDateTimeTransformer\n",
    "            The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Convert specified columns to datetime format in the input DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to be transformed. A pandas DataFrame with columns specified in `cols`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : DataFrame\n",
    "            The transformed DataFrame with specified columns converted to datetime format.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If any of the specified columns are not found in the input DataFrame.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.DataFrame({\n",
    "        ...     'Date': ['2024-01-01', '2024-02-01', '2024-03-01'],\n",
    "        ...     'Value': [10, 20, 30]\n",
    "        ... })\n",
    "        >>> transformer = ToDateTimeTransformer(cols=['Date'])\n",
    "        >>> transformer.transform(data)\n",
    "             Date  Value\n",
    "        0 2024-01-01     10\n",
    "        1 2024-02-01     20\n",
    "        2 2024-03-01     30\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        if not all(col in X_transformed.columns for col in self.cols_):\n",
    "            raise KeyError(f\"Some columns specified in 'cols' are not in the DataFrame.\")\n",
    "        X_transformed[self.cols_] = X_transformed[self.cols_].apply(pd.to_datetime, **self.kwargs_)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class GroupbyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that groups data by specified columns and applies aggregation functions.\n",
    "\n",
    "    This transformer is designed for use in a scikit-learn pipeline to perform group-by\n",
    "    operations on a DataFrame and apply aggregation functions. The resulting DataFrame\n",
    "    can optionally have its column names updated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    by : list or str\n",
    "        Column(s) to group by. Can be a single column name or a list of column names.\n",
    "\n",
    "    agg : dict, list, or None, optional (default=None)\n",
    "        Aggregation functions to apply to the grouped data. If a dict is provided,\n",
    "        keys should be column names and values should be functions or strings specifying\n",
    "        the aggregation. If a list is provided, it should match the order of the columns\n",
    "        in `by`. If None, no aggregation is performed.\n",
    "\n",
    "    names : list or None, optional (default=None)\n",
    "        List of new column names for the transformed DataFrame. If provided, it should\n",
    "        have the same length as the number of columns in the transformed DataFrame.\n",
    "    \n",
    "    reset_index: boolean (default=True)\n",
    "        Specifier to call apply reset_index method or not.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None):\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "    transform(X):\n",
    "        Apply the group-by and aggregation operations to the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The input data to be transformed. A pandas DataFrame on which group-by and\n",
    "        aggregation operations will be applied.\n",
    "\n",
    "    y : None\n",
    "        Ignored. This parameter is included to maintain compatibility with the\n",
    "        scikit-learn transformer API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_transformed : DataFrame\n",
    "        The transformed DataFrame with group-by and aggregation applied. Column names\n",
    "        are updated if `names_` is provided.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    ...     'Value': [10, 20, 30, 40, 50, 60]\n",
    "    ... })\n",
    "    >>> transformer = GroupbyTransformer(by='Category', agg={'Value': 'sum'}, names=['Category', 'TotalValue'])\n",
    "    >>> transformer.fit_transform(data)\n",
    "      Category  TotalValue\n",
    "    0         A          90\n",
    "    1         B         120\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, by, agg=None, names=None, reset_index=True):\n",
    "        \"\"\"\n",
    "        Initialize the GroupbyTransformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        by : list or str\n",
    "            Column(s) to group by. Can be a single column name or a list of column names.\n",
    "\n",
    "        agg : dict, list, or None, optional (default=None)\n",
    "            Aggregation functions to apply to the grouped data. If a dict is provided,\n",
    "            keys should be column names and values should be functions or strings specifying\n",
    "            the aggregation. If a list is provided, it should match the order of the columns\n",
    "            in `by`. If None, no aggregation is performed.\n",
    "\n",
    "        names : list or None, optional (default=None)\n",
    "            List of new column names for the transformed DataFrame. If provided, it should\n",
    "            have the same length as the number of columns in the transformed DataFrame.\n",
    "        \"\"\"\n",
    "        self.by_ = by\n",
    "        self.agg_ = agg\n",
    "        self.names_ = names\n",
    "        self.reset_index_ = reset_index\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data. This transformer does not require fitting,\n",
    "        so it simply returns `self`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to fit. Not used in this transformer.\n",
    "\n",
    "        y : None\n",
    "            Ignored. This parameter is included to maintain compatibility with the\n",
    "            scikit-learn transformer API.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : GroupbyTransformer\n",
    "            The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the group-by and aggregation operations to the input DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to be transformed. A pandas DataFrame on which group-by and\n",
    "            aggregation operations will be applied.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : DataFrame\n",
    "            The transformed DataFrame with group-by and aggregation applied. Column names\n",
    "            are updated if `names_` is provided.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If any of the columns specified in `by` or `agg` are not found in the DataFrame.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.DataFrame({\n",
    "        ...     'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "        ...     'Value': [10, 20, 30, 40, 50, 60]\n",
    "        ... })\n",
    "        >>> transformer = GroupbyTransformer(by='Category', agg={'Value': 'sum'}, names=['Category', 'TotalValue'])\n",
    "        >>> transformer.transform(data)\n",
    "          Category  TotalValue\n",
    "        0         A          90\n",
    "        1         B         120\n",
    "        \"\"\"\n",
    "        X_transformed = X.groupby(by=self.by_).agg(self.agg_).reset_index() if self.reset_index_ else X.groupby(by=self.by_).agg(self.agg_)\n",
    "        if self.names_ is not None:\n",
    "            X_transformed.columns = self.names_\n",
    "        return X_transformed\n",
    "    \n",
    "    \n",
    "class DataFrameMerger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer for joining two tables (DataFrames) based on a specified key.\n",
    "\n",
    "    This transformer is designed to join an additional DataFrame to the input DataFrame\n",
    "    during the transformation step. The join is performed on specified key columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    right : DataFrame\n",
    "        The right DataFrame to join with the input DataFrame.\n",
    "\n",
    "    how : str, optional (default='inner')\n",
    "        Type of join to perform. Options include 'left', 'right', 'outer', 'inner'.\n",
    "\n",
    "    on : str or list\n",
    "        Column or index level names to join on. Must be found in both DataFrames.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None):\n",
    "        This transformer does not require fitting, so it simply returns `self`.\n",
    "\n",
    "    transform(X):\n",
    "        Join the input DataFrame `X` with the right DataFrame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> \n",
    "    >>> # Sample data\n",
    "    >>> left_df = pd.DataFrame({\n",
    "    ...     'id': [1, 2, 3],\n",
    "    ...     'value_left': ['A', 'B', 'C']\n",
    "    ... })\n",
    "    ...\n",
    "    >>> right_df = pd.DataFrame({\n",
    "    ...     'id': [1, 2, 4],\n",
    "    ...     'value_right': ['D', 'E', 'F']\n",
    "    ... })\n",
    "    ...\n",
    "    >>> # Create the transformer\n",
    "    >>> join_transformer = JoinTables(right=right_df, how='left', on='id')\n",
    "    ...\n",
    "    >>> # Apply the transformation\n",
    "    >>> transformed_data = join_transformer.fit_transform(left_df)\n",
    "    >>> print(transformed_data)\n",
    "       id value_left value_right\n",
    "    0   1          A           D\n",
    "    1   2          B           E\n",
    "    2   3          C         NaN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, right, how='inner', on=None):\n",
    "        self.right_ = right\n",
    "        self.how_ = how\n",
    "        self.on_ = on\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This transformer does not require fitting, so it simply returns `self`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to fit. Not used in this transformer.\n",
    "        \n",
    "        y : None, optional\n",
    "            Ignored. This parameter is included to maintain compatibility with the\n",
    "            scikit-learn transformer API.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : JoinTables\n",
    "            The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Join the input DataFrame `X` with the right DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to be transformed. A pandas DataFrame to be joined with\n",
    "            the right DataFrame.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : DataFrame\n",
    "            The DataFrame resulting from the join operation.\n",
    "        \"\"\"\n",
    "        return X.merge(self.right_, how=self.how_, on=self.on_)\n",
    "        \n",
    "\n",
    "class ColumnAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that adds a new column to a DataFrame based on existing columns.\n",
    "\n",
    "    This transformer allows you to create a new column in the input DataFrame by applying\n",
    "    a specified function to one or more existing columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    column_names : list of str\n",
    "        List of column names to be used as input for the function.\n",
    "    \n",
    "    adder : callable\n",
    "        A function that takes the DataFrame and the list of column names as input and returns\n",
    "        a Series or array-like object to be added as a new column.\n",
    "\n",
    "    new_col : str\n",
    "        Name of the new column to be added to the DataFrame.\n",
    "            \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None):\n",
    "        This transformer does not require fitting, so it simply returns `self`.\n",
    "\n",
    "    transform(X):\n",
    "        Add the new column to the input DataFrame by applying the specified function.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> \n",
    "    >>> # Sample data\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'A': [1, 2, 3],\n",
    "    ...     'B': [4, 5, 6]\n",
    "    ... })\n",
    "    ...\n",
    "    >>> # Example function to add a new column\n",
    "    >>> def sum_columns(df, cols):\n",
    "    ...     return df[cols].sum(axis=1)\n",
    "    ...\n",
    "    >>> # Create the transformer\n",
    "    >>> column_adder = ColumnAdder(column_names=['A', 'B'], adder=sum_columns, new_col='A+B')\n",
    "    ...\n",
    "    >>> # Apply the transformation\n",
    "    >>> transformed_df = column_adder.fit_transform(df)\n",
    "    >>> print(transformed_df)\n",
    "       A  B  A+B\n",
    "    0  1  4    5\n",
    "    1  2  5    7\n",
    "    2  3  6    9\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column_names, adder, new_col):\n",
    "        self.cols_ = column_names\n",
    "        self.adder_ = adder\n",
    "        self.new_col_ = new_col\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This transformer does not require fitting, so it simply returns `self`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to fit. Not used in this transformer.\n",
    "        \n",
    "        y : None, optional\n",
    "            Ignored. This parameter is included to maintain compatibility with the\n",
    "            scikit-learn transformer API.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : ColumnAdder\n",
    "            The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Add the new column to the input DataFrame by applying the specified function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            The input data to be transformed. A pandas DataFrame to which the new column\n",
    "            will be added.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : DataFrame\n",
    "            The DataFrame with the new column added.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed.loc[:, self.new_col_] = self.adder_(X, self.cols_)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc340d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_outstanding_merger(df, claims_data):\n",
    "    \"\"\"\n",
    "    A function that merges outstanding amounts from claims data into the main DataFrame.\n",
    "\n",
    "    This function updates the `transferred_amount` and `date_of_trans` columns in the main\n",
    "    DataFrame based on the `claims_data`. For each debt ID, it searches for the closest\n",
    "    contact date in the main DataFrame that is before or on the date of transaction from the \n",
    "    claims data and adds the collected amount to the corresponding row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The main DataFrame that contains details such as `debt_id`, `contact_date`,\n",
    "        and `transferred_amount`.\n",
    "    \n",
    "    claims_data : DataFrame\n",
    "        The DataFrame containing claims information, including `debt_id`, `date_of_trans`,\n",
    "        and `collected_amount`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_transformed : DataFrame\n",
    "        The transformed DataFrame with updated `transferred_amount` and `date_of_trans`\n",
    "        columns based on the claims data.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> # Sample data\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'debt_id': [1, 1, 2, 2],\n",
    "    ...     'contact_date': ['2023-01-01', '2023-02-01', '2023-01-15', '2023-03-01'],\n",
    "    ...     'transferred_amount': [100, 200, 150, 250],\n",
    "    ...     'date_of_trans': [None, None, None, None]\n",
    "    ... })\n",
    "    >>> claims_data = pd.DataFrame({\n",
    "    ...     'debt_id': [1, 2, 2],\n",
    "    ...     'date_of_trans': ['2023-01-10', '2023-01-20', '2023-02-25'],\n",
    "    ...     'collected_amount': [50, 60, 70]\n",
    "    ... })\n",
    "    >>> df['contact_date'] = pd.to_datetime(df['contact_date'])\n",
    "    >>> claims_data['date_of_trans'] = pd.to_datetime(claims_data['date_of_trans'])\n",
    "    >>> result = amount_outstanding_merger(df, claims_data)\n",
    "    >>> print(result)\n",
    "       debt_id contact_date  transferred_amount date_of_trans\n",
    "    0        1   2023-01-01                 150    2023-01-10\n",
    "    1        1   2023-02-01                 200           NaT\n",
    "    2        2   2023-01-15                 210    2023-01-20\n",
    "    3        2   2023-03-01                 320    2023-02-25\n",
    "    \"\"\"\n",
    "\n",
    "    X_transformed = df.copy()\n",
    "    debts_id = claims_data.debt_id.unique()\n",
    "    for debt_id in debts_id:\n",
    "        a = claims_data[claims_data['debt_id'] == debt_id]\n",
    "        b = X_transformed[X_transformed['debt_id'] == debt_id]\n",
    "        for trans in a.index:\n",
    "            index = np.searchsorted(b['contact_date'], a.loc[[trans], 'date_of_trans'])[0]\n",
    "            if index >= len(b) or b.iloc[index].contact_date > a.loc[trans, 'date_of_trans']:\n",
    "                index -= 1\n",
    "            if index >= 0:\n",
    "                X_transformed.loc[b.iloc[[index]].index[0], 'transferred_amount'] += a.loc[trans, 'collected_amount']\n",
    "                X_transformed.loc[b.iloc[[index]].index[0], 'date_of_trans'] = a.loc[trans, 'date_of_trans']\n",
    "                \n",
    "    return X_transformed\n",
    "\n",
    "\n",
    "def get_final_df(X, return_to_close_date=True):\n",
    "    \"\"\"\n",
    "    Filters and transforms a DataFrame by calculating the time difference between\n",
    "    transaction dates and entry dates, and preparing the final DataFrame.\n",
    "\n",
    "    This function first filters the input DataFrame to only include rows where\n",
    "    the `transferred_amount` is greater than or equal to 1. It then calculates \n",
    "    the time difference between `date_of_trans` and `enter_date`, storing this \n",
    "    difference in a new column `to_close_date`. If the `enter_date` is later than \n",
    "    or equal to `date_of_trans`, the `to_close_date` is set to 1 day. The function \n",
    "    then drops unnecessary columns and converts any `timedelta` columns to integer \n",
    "    days.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        The input DataFrame that contains transaction details such as `date_of_trans`,\n",
    "        `enter_date`, and `transferred_amount`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_filtered : DataFrame\n",
    "        The transformed DataFrame after filtering, calculating `to_close_date`, \n",
    "        dropping columns, and converting timedelta columns to integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_filtered = X[X.transferred_amount >= 1].copy()\n",
    "\n",
    "    df_filtered['to_close_date'] = df_filtered['date_of_trans'].copy() - df_filtered['enter_date'].copy()\n",
    "    df_filtered.loc[df_filtered['enter_date'] >= df_filtered['date_of_trans'], 'to_close_date'] = pd.to_timedelta('1 days')\n",
    "    \n",
    "    df_filtered = df_filtered.drop(['date_of_trans', 'amount', 'amount_outstanding', 'debt_id', \n",
    "                                'contact_date', 'service_end_date', 'enter_date'], axis=1)\n",
    "    \n",
    "    for timedelta_col in df_filtered.select_dtypes('timedelta').columns:\n",
    "        df_filtered[timedelta_col] = df_filtered[timedelta_col].dt.days.astype(int, errors='ignore')\n",
    "        \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac34a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_pipeline = Pipeline([\n",
    "    ('enter_and_service_end_date_python_date_trans', ToDateTimeTransformer(cols=['enter_date', 'service_end_date'], errors='coerce')),\n",
    "    ('nan_enter_date_remover', DropBy(lambda X: X['enter_date'].isna()))\n",
    "])\n",
    "\n",
    "\n",
    "claims_pipeline = Pipeline([\n",
    "    ('date_of_trans_python_date_trans', ToDateTimeTransformer(cols=['date_of_trans'], errors='coerce'))\n",
    "])\n",
    "\n",
    "\n",
    "main_pipeline = Pipeline([\n",
    "    ('contact_date_python_date_trans', ToDateTimeTransformer(cols=['contact_date'], errors='coerce')),\n",
    "    ('grouper', GroupbyTransformer(by=['contact_date', 'debt_id'], agg='size', names=list(reversed(actions.columns)) + ['number_of_contact'])),\n",
    "    ('enter_date_merger', DataFrameMerger(collection_pipeline.fit_transform(collections[['debt_id', 'collector_id', 'enter_date', 'service_end_date', 'amount', 'agent_id']]), \n",
    "                                          how='left', on='debt_id')),\n",
    "    ('debt_age_adder', ColumnAdder(['contact_date', 'enter_date'], lambda X, col_names: X[col_names[0]] - X[col_names[1]], 'debt_age')),\n",
    "    ('transferred_amount_initializer', ColumnAdder([], lambda X, col_names: np.zeros((X.shape[0], )), 'transferred_amount')),\n",
    "    ('date_of_trans_initializer', ColumnAdder(['contact_date'], lambda X, col_names: X[col_names[0]], 'date_of_trans')),\n",
    "    ('amount_aoutstanding_merger', DataFrameMerger(collections[['debt_id', 'amount_outstanding']], how='left', on='debt_id')),\n",
    "    ('transferred_amount_finder', FunctionTransformer(amount_outstanding_merger, kw_args={'claims_data': claims_pipeline.fit_transform(claims)})),\n",
    "    ('sorter', FunctionTransformer(pd.DataFrame.sort_values, kw_args={'by': ['debt_id', 'contact_date']})),\n",
    "    (\n",
    "        'transferred_amount_cumsumed_adder', ColumnAdder(\n",
    "                                                ['debt_id'], \n",
    "                                                GroupbyTransformer(\n",
    "                                                    by=['debt_id'], \n",
    "                                                    agg={'transferred_amount': np.cumsum},\n",
    "                                                    reset_index=False\n",
    "                                                ).fit_transform, \n",
    "                                                'transferred_amount_cumsumed'\n",
    "                                             )\n",
    "    ),\n",
    "    ('amount_aoutstanding_finder', ColumnAdder(['amount_outstanding', 'transferred_amount_cumsumed'], lambda X, col_names: X[col_names[0]] - X[col_names[1]], 'amount_outstanding')),\n",
    "    ('transferred_amount_cumsumed', FunctionTransformer(pd.DataFrame.drop, kw_args={'columns': ['transferred_amount_cumsumed']})),\n",
    "    ('to_service_end_date_adder', ColumnAdder(['service_end_date', 'contact_date'], lambda X, col_names: X[col_names[0]] - X[col_names[1]], 'to_service_end_date')),\n",
    "    ('final_df_loader', FunctionTransformer(get_final_df)),\n",
    "    ('imputer', KNNImputer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42eeb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_pipeline.fit_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1daa797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000e+00,  1.7000e+01,  2.0000e+00, ...,  2.0000e+02,\n",
       "        -3.0960e+02,  3.5900e+02],\n",
       "       [ 1.0000e+00,  1.7000e+01,  2.0000e+00, ...,  5.0000e+01,\n",
       "        -1.8702e+03,  4.1400e+02],\n",
       "       [ 1.0000e+00,  1.7000e+01,  2.0000e+00, ...,  5.0000e+01,\n",
       "        -1.5778e+03,  4.4700e+02],\n",
       "       ...,\n",
       "       [ 1.0000e+00,  1.9000e+01,  3.0000e+00, ...,  5.2000e+01,\n",
       "        -6.5000e+01,  2.8000e+01],\n",
       "       [ 1.0000e+00,  1.9000e+01,  3.0000e+00, ...,  6.6800e+01,\n",
       "        -8.4000e+01,  2.8000e+01],\n",
       "       [ 1.0000e+00,  1.7000e+01,  4.6000e+01, ...,  4.0000e+02,\n",
       "        -1.4020e+02,  2.2000e+01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
